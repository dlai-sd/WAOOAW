#!/usr/bin/env python3
"""scripts/test_agent.py

Deterministic Test Agent for WAOOAW.

Runs existing tests on epic branch and reports results.
No AI - just test execution and reporting.

Environment:
- `GITHUB_TOKEN` (required): For posting test results via GitHub CLI
"""

import argparse
import hashlib
import json
import os
import re
import subprocess
import sys
import shutil
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple


DEFAULT_TEST_ROOTS_SMOKE = [
    Path("tests"),
    # These contract tests are stable and don't require services.
    Path("main/Foundation/Architecture/APIGateway/tests"),
]

# "standard" expands into a few known-stable service suites. We intentionally do
# not scan the entire repo by default.
DEFAULT_TEST_ROOTS_STANDARD = [
    *DEFAULT_TEST_ROOTS_SMOKE,
    Path("src/CP/BackEnd/tests"),
    Path("src/gateway/middleware/tests"),
]

DEFAULT_EXCLUDE_SUBSTRINGS = [
    "/performance/",
    "/e2e/",
    "test_integration_docker.py",
    "test_e2e_",
]

DEFAULT_EXCLUDE_FILENAME_TOKENS = [
    "load",
    "perf",
    "benchmark",
]


@dataclass(frozen=True)
class ServiceSuite:
    name: str
    test_roots: List[Path]
    requirements_files: List[Path]
    pythonpath_entries: List[Path]


def discover_service_suites(repo_root: Path) -> List[ServiceSuite]:
    """Return deterministic per-service test suites.

    We intentionally keep this explicit (not a full repo scan) to avoid
    accidentally pulling in experimental suites with extra infra needs.
    """

    suites: List[ServiceSuite] = []

    def add_suite(
        name: str,
        *,
        test_roots: List[str],
        requirements_files: List[str],
        pythonpath_entries: List[str],
    ) -> None:
        suites.append(
            ServiceSuite(
                name=name,
                test_roots=[Path(p) for p in test_roots],
                requirements_files=[Path(p) for p in requirements_files],
                pythonpath_entries=[Path(p) for p in pythonpath_entries],
            )
        )

    # Repo-level smoke tests (rare, but keep for sanity).
    add_suite(
        "repo-root",
        test_roots=["tests"],
        requirements_files=["tests/requirements.txt"],
        pythonpath_entries=["."],
    )

    # Customer Portal backend
    add_suite(
        "cp-backend",
        test_roots=["src/CP/BackEnd/tests"],
        requirements_files=["src/CP/BackEnd/requirements.txt"],
        pythonpath_entries=["src/CP/BackEnd"],
    )

    # Plant backend
    add_suite(
        "plant-backend",
        test_roots=["src/Plant/BackEnd/tests"],
        requirements_files=["src/Plant/BackEnd/requirements.txt"],
        pythonpath_entries=["src/Plant/BackEnd"],
    )

    # API Gateway middleware (stable unit/contract-style tests)
    add_suite(
        "gateway-middleware",
        test_roots=["src/gateway/middleware/tests"],
        requirements_files=["src/gateway/requirements.txt"],
        pythonpath_entries=["src/gateway", "src/gateway/middleware"],
    )

    # API Gateway integration suite (requires gateway services running)
    add_suite(
        "gateway-integration",
        test_roots=["src/gateway/tests"],
        requirements_files=["src/gateway/requirements.txt"],
        pythonpath_entries=["src/gateway"],
    )

    # Plant Gateway middleware
    add_suite(
        "plant-gateway",
        test_roots=["src/Plant/Gateway/middleware/tests"],
        requirements_files=["src/Plant/Gateway/requirements.txt"],
        pythonpath_entries=["src/Plant/Gateway", "src/Plant/Gateway/middleware"],
    )

    # Foundation contract tests
    add_suite(
        "foundation-api-gateway",
        test_roots=["main/Foundation/Architecture/APIGateway/tests"],
        requirements_files=["tests/requirements.txt"],
        pythonpath_entries=["."],
    )

    # Resolve and filter to existing paths.
    resolved: List[ServiceSuite] = []
    def _has_tests(root: Path) -> bool:
        if not root.exists():
            return False
        if root.is_file():
            return root.name.startswith("test_") or root.name.endswith("_test.py")
        for pattern in ["test_*.py", "*_test.py"]:
            if any(root.rglob(pattern)):
                return True
        return False

    for suite in suites:
        existing_tests = [repo_root / p for p in suite.test_roots if _has_tests(repo_root / p)]
        if not existing_tests:
            continue

        existing_reqs = [repo_root / p for p in suite.requirements_files if (repo_root / p).exists()]
        if not existing_reqs:
            # A suite without requirements is allowed; it may rely on base deps.
            existing_reqs = []

        resolved.append(
            ServiceSuite(
                name=suite.name,
                test_roots=[p.relative_to(repo_root) for p in existing_tests],
                requirements_files=[p.relative_to(repo_root) for p in existing_reqs],
                pythonpath_entries=suite.pythonpath_entries,
            )
        )

    return resolved


def _iter_test_files_under(root: Path) -> Iterable[Path]:
    if not root.exists():
        return []
    tests: List[Path] = []
    for pattern in ["test_*.py", "*_test.py"]:
        tests.extend(root.rglob(pattern))
    return tests


def find_python_tests(repo_root: Path) -> List[Path]:
    """Find deterministic Python test files.

    Note: We intentionally avoid scanning the entire repository (e.g. `src/**/tests`)
    because many component test suites require additional per-service configuration
    and can break epic automation runs.
    """

    tests: List[Path] = []

    # Repo-root smoke tests (non-recursive)
    for pattern in ["test_*.py", "*_test.py"]:
        tests.extend(repo_root.glob(pattern))

    scope = os.getenv("WAOOAW_TEST_SCOPE", "standard").strip().lower()
    if scope not in {"smoke", "standard"}:
        scope = "standard"

    selected_roots = (
        DEFAULT_TEST_ROOTS_SMOKE if scope == "smoke" else DEFAULT_TEST_ROOTS_STANDARD
    )

    # Selected stable test roots
    for rel_root in selected_roots:
        tests.extend(_iter_test_files_under(repo_root / rel_root))

    # Optional extra paths (colon-separated), e.g. "src/some/module/tests:other_tests"
    extra_paths = os.getenv("WAOOAW_EXTRA_TEST_PATHS", "").strip()
    if extra_paths:
        for raw in extra_paths.split(":"):
            path = (repo_root / raw.strip()).resolve()
            try:
                path.relative_to(repo_root)
            except ValueError:
                # Skip paths outside the repo root.
                continue
            tests.extend(_iter_test_files_under(path))

    exclude_substrings = list(DEFAULT_EXCLUDE_SUBSTRINGS)
    extra_exclude_substrings = os.getenv("WAOOAW_TEST_EXCLUDE_SUBSTRINGS", "").strip()
    if extra_exclude_substrings:
        exclude_substrings.extend([s.strip() for s in extra_exclude_substrings.split(":") if s.strip()])

    exclude_filename_tokens = list(DEFAULT_EXCLUDE_FILENAME_TOKENS)
    extra_exclude_tokens = os.getenv("WAOOAW_TEST_EXCLUDE_FILENAME_TOKENS", "").strip()
    if extra_exclude_tokens:
        exclude_filename_tokens.extend([s.strip() for s in extra_exclude_tokens.split(":") if s.strip()])

    # Exclude old agent backups and de-dup while preserving order
    seen: set[str] = set()
    ordered: List[Path] = []
    for test_path in tests:
        as_posix = test_path.as_posix()
        if "_old.py" in as_posix:
            continue
        if as_posix in seen:
            continue
        seen.add(as_posix)
        ordered.append(test_path)

    # Apply exclusion filters last (keeps deterministic ordering)
    filtered: List[Path] = []
    for test_path in ordered:
        as_posix = test_path.as_posix()
        lower_name = test_path.name.lower()

        if any(s in as_posix for s in exclude_substrings):
            continue
        if any(tok in lower_name for tok in exclude_filename_tokens):
            continue
        filtered.append(test_path)

    return filtered


def find_test_suites(repo_root: Path) -> List[ServiceSuite]:
    scope = os.getenv("WAOOAW_TEST_SCOPE", "standard").strip().lower()
    if scope not in {"smoke", "standard", "full"}:
        scope = "standard"

    suites = discover_service_suites(repo_root)
    if scope == "smoke":
        selected = [s for s in suites if s.name in {"repo-root", "foundation-api-gateway"}]
    elif scope == "standard":
        # Skip suites that require additional running services/secrets.
        selected = [
            s
            for s in suites
            if s.name
            not in {
                "gateway-integration",
                # Plant backend suite currently contains strict regression tests that
                # are not yet stable for epic automation gating.
                "plant-backend",
            }
        ]
    else:
        # full
        selected = suites

    # Optional explicit suite filter (colon-separated), e.g. "plant-backend:cp-backend".
    only_raw = os.getenv("WAOOAW_ONLY_SUITES", "").strip()
    if only_raw:
        allowed = {s.strip() for s in only_raw.split(":") if s.strip()}
        selected = [s for s in selected if s.name in allowed]

    return selected


def run_pytest(test_files: List[Path]) -> Tuple[bool, Dict, str]:
    """Run pytest and parse results.
    
    Returns:
        (success, summary_dict, output)
    """
    if not test_files:
        return True, {"status": "no_tests"}, "No Python tests found"
    
    try:
        mode = os.getenv("WAOOAW_TEST_EXECUTION_MODE", "docker").strip().lower()
        if mode == "docker" and shutil.which("docker"):
            result = _run_pytest_in_docker(test_files)
        else:
            cmd = _build_pytest_cmd(test_files, enforce_cov=_enforce_coverage())

            print("[Test Agent] Executing pytest command:")
            print("[Test Agent] " + " ".join(cmd))

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
        
        output = result.stdout + "\n" + result.stderr
        
        # Parse summary from output
        summary = {
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "errors": 0,
            "total": 0,
            "coverage_total": None,
        }

        # Pytest exit code 5 commonly means "no tests collected".
        # This is usually a selection/scoping issue, so surface it explicitly.
        if result.returncode == 5 and ("collected 0 items" in output.lower()):
            summary["status"] = "no_tests_collected"
        
        # Parse counts from any output line (handles collection errors / no-summary cases)
        count_patterns = {
            "passed": re.compile(r"(?P<count>\d+)\s+passed", re.IGNORECASE),
            "failed": re.compile(r"(?P<count>\d+)\s+failed", re.IGNORECASE),
            "skipped": re.compile(r"(?P<count>\d+)\s+skipped", re.IGNORECASE),
            "errors": re.compile(r"(?P<count>\d+)\s+errors?", re.IGNORECASE),
        }

        for line in output.split("\n"):
            for key, pattern in count_patterns.items():
                match = pattern.search(line)
                if match:
                    summary[key] = int(match.group("count"))

            if line.startswith("TOTAL") and "%" in line:
                # e.g. "TOTAL 123 4 97%"
                parts = line.split()
                for part in reversed(parts):
                    if part.endswith("%"):
                        try:
                            summary["coverage_total"] = float(part.rstrip("%"))
                        except ValueError:
                            summary["coverage_total"] = None
                        break
        
        summary["total"] = summary["passed"] + summary["failed"] + summary["skipped"] + summary["errors"]
        summary["returncode"] = result.returncode
        success = result.returncode == 0
        
        return success, summary, output
        
    except subprocess.TimeoutExpired:
        return False, {"status": "timeout"}, "Tests timed out after 10 minutes"
    except FileNotFoundError:
        return False, {"status": "not_installed"}, "pytest not installed"
    except Exception as e:
        return False, {"status": "error"}, f"Test execution failed: {e}"


def run_pytest_per_service(suites: List[ServiceSuite]) -> Tuple[bool, Dict, str]:
    if not suites:
        only_raw = os.getenv("WAOOAW_ONLY_SUITES", "").strip()
        if only_raw:
            return (
                False,
                {"status": "no_suites_matched", "only_suites": only_raw},
                f"WAOOAW_ONLY_SUITES did not match any suites: {only_raw}",
            )
        return True, {"status": "no_tests"}, "No service test suites found"

    mode = os.getenv("WAOOAW_TEST_EXECUTION_MODE", "docker").strip().lower()
    if mode != "docker" or not shutil.which("docker"):
        return False, {"status": "unsupported"}, "Per-service runs require docker (set WAOOAW_TEST_EXECUTION_MODE=docker)"

    overall_success = True
    suite_summaries: List[Dict] = []
    combined_output_parts: List[str] = []

    for suite in suites:
        ok, summary, output = _run_suite_in_docker(suite)
        overall_success = overall_success and ok

        suite_summaries.append(
            {
                "name": suite.name,
                "ok": ok,
                **summary,
            }
        )
        combined_output_parts.append(f"\n===== SUITE: {suite.name} =====\n")
        combined_output_parts.append(output)

    # Aggregate high-level counters.
    agg = {
        "status": "ok" if overall_success else "failed",
        "suites": suite_summaries,
        "passed": sum(int(s.get("passed", 0)) for s in suite_summaries),
        "failed": sum(int(s.get("failed", 0)) for s in suite_summaries),
        "skipped": sum(int(s.get("skipped", 0)) for s in suite_summaries),
        "errors": sum(int(s.get("errors", 0)) for s in suite_summaries),
    }
    agg["total"] = agg["passed"] + agg["failed"] + agg["skipped"] + agg["errors"]

    return overall_success, agg, "\n".join(combined_output_parts)


def _parse_pytest_output(output: str, returncode: int) -> Dict:
    summary = {
        "passed": 0,
        "failed": 0,
        "skipped": 0,
        "errors": 0,
        "total": 0,
        "coverage_total": None,
        "returncode": returncode,
    }

    if returncode == 5 and ("collected 0 items" in output.lower()):
        summary["status"] = "no_tests_collected"

    count_patterns = {
        "passed": re.compile(r"(?P<count>\d+)\s+passed\b", re.IGNORECASE),
        "failed": re.compile(r"(?P<count>\d+)\s+failed\b", re.IGNORECASE),
        "skipped": re.compile(r"(?P<count>\d+)\s+skipped\b", re.IGNORECASE),
        "errors": re.compile(r"(?P<count>\d+)\s+errors?\b", re.IGNORECASE),
    }

    # Only parse counts from pytest's final summary line(s). Otherwise we can
    # accidentally match unrelated lines like "Connect call failed ('127.0.0.1', 5432)".
    summary_lines: List[str] = []
    for line in output.split("\n"):
        stripped = line.strip()
        if not stripped.startswith("="):
            continue
        if "passed" in stripped.lower() or "failed" in stripped.lower() or "error" in stripped.lower():
            summary_lines.append(stripped)

    for line in summary_lines[-3:]:
        for key, pattern in count_patterns.items():
            match = pattern.search(line)
            if match:
                summary[key] = int(match.group("count"))

        if line.startswith("TOTAL") and "%" in line:
            parts = line.split()
            for part in reversed(parts):
                if part.endswith("%"):
                    try:
                        summary["coverage_total"] = float(part.rstrip("%"))
                    except ValueError:
                        summary["coverage_total"] = None
                    break

    summary["total"] = (
        summary["passed"] + summary["failed"] + summary["skipped"] + summary["errors"]
    )
    return summary


def _docker_volume_name_for_suite(suite_name: str) -> str:
    safe = re.sub(r"[^a-z0-9]+", "-", suite_name.lower()).strip("-")
    return f"waooaw_pydeps_{safe}"


def _run_suite_in_docker(suite: ServiceSuite) -> Tuple[bool, Dict, str]:
    repo_root = Path.cwd().resolve()
    enforce_cov = _enforce_coverage()
    collect_only_raw = os.getenv("WAOOAW_PYTEST_COLLECT_ONLY", "").strip().lower()
    collect_only = collect_only_raw in {"1", "true", "yes", "on"}

    # Fingerprint suite deps so we can cache installs in the suite's /deps volume.
    hasher = hashlib.sha256()
    for req in suite.requirements_files:
        try:
            hasher.update((repo_root / req).read_bytes())
        except Exception:
            hasher.update(req.as_posix().encode("utf-8"))
    hasher.update(b"|pytest pytest-asyncio pytest-cov pytest-mock pytest-timeout|")
    deps_fingerprint = hasher.hexdigest()[:16]
    marker_file = f"/deps/.waooaw_deps_{suite.name}_{deps_fingerprint}"

    # Build a deterministic, per-suite PYTHONPATH.
    pythonpath_entries = [Path("/deps")]
    for rel in suite.pythonpath_entries:
        pythonpath_entries.append(Path("/repo") / rel)
    pythonpath = ":".join(p.as_posix() for p in pythonpath_entries)

    # Install deps into a per-suite cached volume to avoid cross-service pin conflicts.
    volume = _docker_volume_name_for_suite(suite.name)
    install_lines: List[str] = [
        "set -euo pipefail",
        "export PIP_DISABLE_PIP_VERSION_CHECK=1",
        "export PIP_ROOT_USER_ACTION=ignore",
        f"if [ ! -f '{marker_file}' ]; then",
        "  python -m pip install -q --disable-pip-version-check --root-user-action=ignore -t /deps -U pip setuptools wheel",
    ]
    for req in suite.requirements_files:
        install_lines.append(
            f"  python -m pip install -q --disable-pip-version-check --root-user-action=ignore -t /deps -r /repo/{req.as_posix()}"
        )

    # Some services omit test runners from their runtime requirements.
    install_lines.append(
        "  python -m pip install -q --disable-pip-version-check --root-user-action=ignore -t /deps "
        "pytest pytest-asyncio pytest-cov pytest-mock pytest-timeout"
    )
    install_lines.extend([f"  touch '{marker_file}'", "fi"])

    # Build pytest command with explicit paths (avoids relying on repo-root pytest.ini discovery).
    pytest_args: List[str] = [
        "python",
        "-m",
        "pytest",
        "-v",
        "--tb=short",
        "--maxfail=5",
        "--import-mode=importlib",
    ]

    if collect_only:
        pytest_args.append("--collect-only")

    include_perf_raw = os.getenv("WAOOAW_TEST_INCLUDE_PERF", "").strip().lower()
    include_perf = include_perf_raw in {"1", "true", "yes", "on"}

    include_integration_raw = os.getenv("WAOOAW_TEST_INCLUDE_INTEGRATION", "").strip().lower()
    include_integration = include_integration_raw in {"1", "true", "yes", "on"}
    if not include_perf:
        pytest_args.extend(
            [
                "--ignore-glob=*load*.py",
                "--ignore-glob=*perf*.py",
                "--ignore-glob=*benchmark*.py",
                "--ignore-glob=test_integration_docker.py",
                "--ignore-glob=test_e2e_*.py",
                "--ignore-glob=*deprecated*",
            ]
        )

        # Explicit ignores (more reliable than ignore-glob across nested roots)
        explicit_ignores: List[str] = []
        for root in suite.test_roots:
            host_root = repo_root / root
            if not host_root.exists():
                continue

            # E2E files
            for p in list(host_root.rglob("test_e2e_*.py"))[:50]:
                try:
                    rel = p.relative_to(repo_root)
                    explicit_ignores.append(f"--ignore=/repo/{rel.as_posix()}")
                except ValueError:
                    continue

            # Docker-only integration files
            for p in list(host_root.rglob("test_integration_docker.py"))[:50]:
                try:
                    rel = p.relative_to(repo_root)
                    explicit_ignores.append(f"--ignore=/repo/{rel.as_posix()}")
                except ValueError:
                    continue

        pytest_args.extend(explicit_ignores)

    if os.getenv("WAOOAW_COVERAGE", "").strip().lower() in {"1", "true", "yes", "on"}:
        target = os.getenv("WAOOAW_COVERAGE_TARGET", "src").strip() or "src"
        pytest_args.extend([f"--cov={target}", "--cov-report=term-missing"])

        cov_min_raw = os.getenv("WAOOAW_COVERAGE_MIN", "").strip()
        if cov_min_raw:
            try:
                cov_min = int(cov_min_raw)
            except ValueError:
                cov_min = 0
            if enforce_cov and cov_min > 0:
                pytest_args.append(f"--cov-fail-under={cov_min}")

    if not enforce_cov:
        pytest_args.append("--cov-fail-under=0")

    for root in suite.test_roots:
        container_root = f"/repo/{root.as_posix()}"
        if not include_perf:
            # Skip common heavy suites if present.
            perf_dir = f"{container_root}/performance"
            e2e_dir = f"{container_root}/e2e"
            pytest_args.extend([f"--ignore={perf_dir}", f"--ignore={e2e_dir}"])

        if not include_integration:
            integration_dir = f"{container_root}/integration"
            pytest_args.append(f"--ignore={integration_dir}")

        pytest_args.append(container_root)

    # Per-suite stable defaults (kept minimal; CI should not rely on developer DBs).
    suite_env: Dict[str, str] = {}
    default_db_url = "postgresql+asyncpg://waooaw_test:waooaw_test_password@localhost:5433/waooaw_test_db"
    default_redis_url = "redis://localhost:6380/0"

    if suite.name in {"plant-backend", "cp-backend"}:
        suite_env.setdefault("DATABASE_URL", os.getenv("DATABASE_URL") or default_db_url)
        suite_env.setdefault("REDIS_URL", os.getenv("REDIS_URL") or default_redis_url)
        suite_env.setdefault("ENVIRONMENT", os.getenv("ENVIRONMENT") or "test")

    cmd: List[str] = [
        "docker",
        "run",
        "--rm",
        "--network",
        "host",
        "-v",
        f"{repo_root.as_posix()}:/repo",
        "-v",
        f"{volume}:/deps",
        "-w",
        "/repo",
        "-e",
        f"TEST_MODE={os.getenv('TEST_MODE', '')}",
        "-e",
        f"TEST_DATABASE_URL={os.getenv('TEST_DATABASE_URL', '')}",
        "-e",
        f"TEST_REDIS_URL={os.getenv('TEST_REDIS_URL', '')}",
        "-e",
        f"WAOOAW_TEST_SCOPE={os.getenv('WAOOAW_TEST_SCOPE', '')}",
        "-e",
        f"WAOOAW_COVERAGE={os.getenv('WAOOAW_COVERAGE', '')}",
        "-e",
        f"WAOOAW_COVERAGE_TARGET={os.getenv('WAOOAW_COVERAGE_TARGET', '')}",
        "-e",
        f"WAOOAW_COVERAGE_MIN={os.getenv('WAOOAW_COVERAGE_MIN', '')}",
        "-e",
        f"WAOOAW_ENFORCE_COVERAGE={os.getenv('WAOOAW_ENFORCE_COVERAGE', '')}",
        "-e",
        f"DATABASE_URL={suite_env.get('DATABASE_URL', os.getenv('DATABASE_URL', ''))}",
        "-e",
        f"REDIS_URL={suite_env.get('REDIS_URL', os.getenv('REDIS_URL', ''))}",
        "-e",
        f"ENVIRONMENT={suite_env.get('ENVIRONMENT', os.getenv('ENVIRONMENT', ''))}",
        "-e",
        f"PYTHONPATH={pythonpath}",
        "python:3.11-slim",
        "bash",
        "-lc",
        "\n".join(install_lines + [" ".join(pytest_args)]),
    ]

    print(f"[Test Agent] Running suite: {suite.name}")
    print(f"[Test Agent] - Test roots: {', '.join(r.as_posix() for r in suite.test_roots)}")
    if suite.requirements_files:
        print(
            f"[Test Agent] - Requirements: {', '.join(r.as_posix() for r in suite.requirements_files)}"
        )

    result = subprocess.run(cmd, capture_output=True, text=True, timeout=1200)
    output = (result.stdout or "") + "\n" + (result.stderr or "")
    summary = _parse_pytest_output(output, result.returncode)
    ok = result.returncode == 0
    return ok, summary, output


def _enforce_coverage() -> bool:
    enforce_cov_raw = os.getenv("WAOOAW_ENFORCE_COVERAGE", "").strip().lower()
    return enforce_cov_raw in {"1", "true", "yes", "on"}


def _build_pytest_cmd(test_files: List[Path], *, enforce_cov: bool) -> List[str]:
    cmd: List[str] = [
        sys.executable,
        "-m",
        "pytest",
        "-v",
        "--tb=short",
        "--maxfail=5",
        *[str(p) for p in test_files],
    ]

    collect_only_raw = os.getenv("WAOOAW_PYTEST_COLLECT_ONLY", "").strip().lower()
    if collect_only_raw in {"1", "true", "yes", "on"}:
        cmd.append("--collect-only")

    # Optional coverage: set WAOOAW_COVERAGE=1 and optionally WAOOAW_COVERAGE_MIN
    if os.getenv("WAOOAW_COVERAGE", "").strip().lower() in {"1", "true", "yes", "on"}:
        target = os.getenv("WAOOAW_COVERAGE_TARGET", "src").strip() or "src"
        cmd.extend([f"--cov={target}", "--cov-report=term-missing"])

        cov_min_raw = os.getenv("WAOOAW_COVERAGE_MIN", "").strip()
        if cov_min_raw:
            try:
                cov_min = int(cov_min_raw)
            except ValueError:
                cov_min = 0
            if enforce_cov and cov_min > 0:
                cmd.append(f"--cov-fail-under={cov_min}")

    # Default behavior: do NOT let coverage thresholds block the epic when a subset ran.
    # This neutralizes per-service pytest.ini settings like `--cov-fail-under=76`.
    if not enforce_cov:
        cmd.append("--cov-fail-under=0")

    return cmd


def _run_pytest_in_docker(test_files: List[Path]) -> subprocess.CompletedProcess[str]:
    repo_root = Path.cwd().resolve()
    dockerfile = repo_root / "tests" / "Dockerfile.test"
    image = os.getenv("WAOOAW_DOCKER_TEST_IMAGE", "waooaw-test-runner:py311").strip()
    container_repo_root = Path("/repo")

    if not dockerfile.exists():
        raise FileNotFoundError(f"Missing {dockerfile.as_posix()}")

    # Build (cached) test runner image.
    subprocess.run(
        [
            "docker",
            "build",
            "-f",
            dockerfile.as_posix(),
            "-t",
            image,
            repo_root.as_posix(),
        ],
        check=True,
        capture_output=True,
        text=True,
    )

    enforce_cov = _enforce_coverage()

    # The selected test files are discovered on the host runner (e.g. under
    # `/home/runner/work/...`). When running inside Docker, the repository is
    # mounted at `/repo`, so we must translate those paths to container-visible
    # paths (e.g. `/repo/src/...`).
    container_test_files: List[Path] = []
    for test_path in test_files:
        try:
            resolved = test_path.resolve()
            rel = resolved.relative_to(repo_root)
            container_test_files.append(container_repo_root / rel)
            continue
        except Exception:
            pass

        if not test_path.is_absolute():
            container_test_files.append(container_repo_root / test_path)
        else:
            # Fallback: keep as-is (may still fail, but preserves diagnostics).
            container_test_files.append(test_path)

    pytest_cmd = _build_pytest_cmd(container_test_files, enforce_cov=enforce_cov)
    # Replace the leading interpreter invocation; inside the container we always call `python`.
    pytest_cmd[0:1] = ["python"]

    cmd: List[str] = [
        "docker",
        "run",
        "--rm",
        "--network",
        "host",
        "-v",
        f"{repo_root.as_posix()}:/repo",
        "-w",
        "/repo",
        # Pass through test/runtime env vars
        "-e",
        f"TEST_MODE={os.getenv('TEST_MODE', '')}",
        "-e",
        f"TEST_DATABASE_URL={os.getenv('TEST_DATABASE_URL', '')}",
        "-e",
        f"TEST_REDIS_URL={os.getenv('TEST_REDIS_URL', '')}",
        "-e",
        f"WAOOAW_TEST_SCOPE={os.getenv('WAOOAW_TEST_SCOPE', '')}",
        "-e",
        f"WAOOAW_COVERAGE={os.getenv('WAOOAW_COVERAGE', '')}",
        "-e",
        f"WAOOAW_COVERAGE_TARGET={os.getenv('WAOOAW_COVERAGE_TARGET', '')}",
        "-e",
        f"WAOOAW_COVERAGE_MIN={os.getenv('WAOOAW_COVERAGE_MIN', '')}",
        "-e",
        f"WAOOAW_ENFORCE_COVERAGE={os.getenv('WAOOAW_ENFORCE_COVERAGE', '')}",
        "-e",
        "PYTHONPATH=.",
        image,
        *pytest_cmd,
    ]

    return subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        timeout=600,
    )


def format_test_summary(success: bool, summary: Dict, output: str) -> str:
    """Format test results as markdown."""
    
    status_emoji = "‚úÖ" if success else "‚ùå"
    title = f"## {status_emoji} Test Agent Results\n\n"
    
    if summary.get("status") == "no_tests":
        return title + "No tests found in configured service suites.\n"

    if summary.get("status") == "no_tests_collected":
        # Keep this as a failure, but make the root cause obvious.
        result = title
        result += "**Status**: ‚ùå Pytest collected 0 tests (selection/scope issue)\n\n"
        result += "### ‚ùå Action Required\n\n"
        result += "Pytest exited with code 5 (no tests collected). This usually means the Test Agent selected paths with no runnable tests, or collection failed before discovery.\n\n"
        result += "### Failure Output (tail)\n\n"
        result += "```\n"
        lines = [l for l in output.split("\n") if l is not None]
        tail = lines[-120:] if len(lines) > 120 else lines
        result += "\n".join(tail).strip() + "\n"
        result += "```\n"
        return result
    
    if summary.get("status") == "not_installed":
        return title + "‚ö†Ô∏è pytest not installed. Install with: `pip install pytest`\n"
    
    if summary.get("status") == "timeout":
        return title + "‚è±Ô∏è Tests timed out after 10 minutes.\n"
    
    if summary.get("status") == "unsupported":
        result = title
        result += "‚ùå Unsupported test execution mode for per-service runs.\n\n"
        result += "```\n"
        result += (output or "").strip()[:2000] + "\n"
        result += "```\n"
        return result

    if summary.get("status") == "error":
        return title + f"‚ùå Error: {output[:500]}\n"
    
    # Format normal results
    passed = summary.get("passed", 0)
    failed = summary.get("failed", 0)
    skipped = summary.get("skipped", 0)
    total = summary.get("total", 0)
    
    result = title
    scope = os.getenv("WAOOAW_TEST_SCOPE", "standard")
    mode = os.getenv("WAOOAW_TEST_EXECUTION_MODE", "docker")
    automation_ref = (
        os.getenv("WAOOAW_AUTOMATION_REF_USED", "").strip()
        or os.getenv("WAOOAW_AUTOMATION_REF", "").strip()
    )
    coverage_gate = "ENFORCED" if _enforce_coverage() else "RELAXED (subset-friendly)"
    result += f"**Execution**: {mode}\\n"
    if automation_ref:
        result += f"**Automation ref**: {automation_ref}\\n"
    result += f"**Scope**: {scope}\\n"
    result += f"**Coverage gate**: {coverage_gate}\\n\\n"
    result += f"**Status**: {'‚úÖ All tests passed' if success else '‚ùå Some tests failed'}\n\n"
    result += f"**Results**:\n"
    result += f"- ‚úÖ Passed: {passed}\n"
    result += f"- ‚ùå Failed: {failed}\n"
    result += f"- ‚è≠Ô∏è Skipped: {skipped}\n"
    result += f"- üìä Total: {total}\n\n"

    suites = summary.get("suites")
    if isinstance(suites, list) and suites:
        result += "### Suites\n\n"
        for s in suites:
            name = str(s.get("name", "suite"))
            ok = bool(s.get("ok", False))
            status = "‚úÖ" if ok else "‚ùå"
            p = int(s.get("passed", 0) or 0)
            f = int(s.get("failed", 0) or 0)
            e = int(s.get("errors", 0) or 0)
            sk = int(s.get("skipped", 0) or 0)
            result += f"- {status} `{name}`: {p} passed, {f} failed, {e} errors, {sk} skipped\n"
        result += "\n"

    cov_total = summary.get("coverage_total")
    if cov_total is not None:
        result += f"**Coverage (TOTAL)**: {cov_total:.1f}%\n\n"
    
    if not success:
        returncode = summary.get("returncode")
        if returncode is not None:
            result += f"**Pytest exit code**: {returncode}\n\n"

        result += "### Failure Output (tail)\n\n"
        result += "```\n"
        lines = [l for l in output.split("\n") if l is not None]
        tail = lines[-120:] if len(lines) > 120 else lines
        result += "\n".join(tail).strip() + "\n"
        result += "```\n\n"
    
    # Next steps
    if success:
        result += "### ‚úÖ Next Steps\n\n"
        result += "All tests passed! Deploy Agent will create a PR to merge this epic.\n"
    else:
        result += "### ‚ùå Action Required\n\n"
        result += "Tests failed. Please fix failing tests before deployment.\n"
        result += "Epic will be labeled with `needs-fix`.\n"
    
    return result


def post_comment_to_epic(epic_number: str, comment: str) -> bool:
    """Post comment to epic using GitHub CLI."""
    try:
        subprocess.run(
            ["gh", "issue", "comment", epic_number, "--body", comment],
            check=True,
            capture_output=True,
            text=True
        )
        return True
    except subprocess.CalledProcessError as e:
        print(f"[ERROR] Failed to post comment: {e.stderr}")
        return False


def _skip_github_posting() -> bool:
    raw = os.getenv("WAOOAW_SKIP_GH_POST", "").strip().lower()
    return raw in {"1", "true", "yes", "on"}


def add_label_to_epic(epic_number: str, label: str) -> bool:
    """Add label to epic using GitHub CLI."""
    try:
        subprocess.run(
            ["gh", "issue", "edit", epic_number, "--add-label", label],
            check=True,
            capture_output=True,
            text=True
        )
        return True
    except subprocess.CalledProcessError as e:
        print(f"[ERROR] Failed to add label: {e.stderr}")
        return False


def main() -> None:
    parser = argparse.ArgumentParser(description="Deterministic Test Agent for WAOOAW")
    parser.add_argument("--epic-number", required=True, help="Epic issue number")
    parser.add_argument("--epic-branch", required=True, help="Epic branch name")
    args = parser.parse_args()
    
    epic_number = str(args.epic_number)
    epic_branch = str(args.epic_branch)
    
    print(f"[Test Agent] Running tests for Epic #{epic_number}")
    print(f"[Test Agent] Branch: {epic_branch}")
    
    repo_root = Path.cwd()
    suites = find_test_suites(repo_root)
    print(f"[Test Agent] Selected {len(suites)} per-service suite(s)")
    for s in suites:
        print(
            f"[Test Agent] - {s.name}: "
            + ", ".join(r.as_posix() for r in s.test_roots)
        )

    success, summary, output = run_pytest_per_service(suites)
    
    # Format summary
    comment = format_test_summary(success, summary, output)
    
    print("[Test Agent] Test Results:")
    print(comment)
    
    # Post to GitHub (optional)
    if _skip_github_posting():
        print("[Test Agent] Skipping GitHub comment posting (WAOOAW_SKIP_GH_POST=1)")
    else:
        print(f"[Test Agent] Posting results to Epic #{epic_number}...")
        if not post_comment_to_epic(epic_number, comment):
            print("[ERROR] Failed to post results to GitHub")
            sys.exit(1)
    
    # Add label if failed
    if not success:
        if _skip_github_posting():
            print("[Test Agent] Skipping GitHub labeling (WAOOAW_SKIP_GH_POST=1)")
        else:
            print(f"[Test Agent] Adding 'needs-fix' label to Epic #{epic_number}")
            add_label_to_epic(epic_number, "needs-fix")
        print("[Test Agent] ‚ùå Tests failed")
        sys.exit(1)
    
    print("[Test Agent] ‚úÖ All tests passed!")


if __name__ == "__main__":
    main()
