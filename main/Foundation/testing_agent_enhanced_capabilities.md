# Testing Agent Enhanced Capabilities
## Security Testing + Performance Testing + Core Testing Enhancements

**Agent ID**: TEST-PLT-001 Enhanced  
**Version**: 2.0  
**Last Updated**: January 19, 2026  
**New Capabilities**: Security Agent + Performance Agent expertise integrated

---

## ðŸ”’ SECURITY TESTING CAPABILITIES (Security Agent Expertise)

### 1. Threat Modeling & Security Architecture Review
**Triggered**: When Architect completes security analysis for epic

**Activities**:
- Review STRIDE threat model (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege)
- Identify attack surfaces across all components (APIs, UI, databases, message queues, integrations)
- Map data flows for sensitive data (PII, payment info, JWT tokens, API keys)
- Assess authentication mechanisms (OAuth, JWT, session management)
- Review authorization controls (RBAC, attribute-based access)
- Validate secrets management (rotation, encryption at rest, no hardcoded secrets)

### 2. OWASP Top 10 Automated Testing
**Security Scan Suite** (run before every deployment):
```bash
# A01: Broken Access Control
python3 tests/security/test_access_control.py

# A02: Cryptographic Failures  
python3 tests/security/test_encryption.py

# A03: SQL Injection
sqlmap -u "https://demo.waooaw.com/api/agents?id=1" --batch --level=3

# A04: Insecure Design
# Manual review of architecture decisions

# A05: Security Misconfiguration
python3 tests/security/test_security_headers.py
python3 tests/security/test_cors_policy.py

# A06: Vulnerable Components
safety check --json | python3 tests/security/check_vulnerabilities.py
npm audit --json | python3 tests/security/check_npm_vulns.py
snyk test --severity-threshold=high

# A07: Authentication Failures
python3 tests/security/test_auth_bypasses.py
python3 tests/security/test_session_management.py

# A08: Software/Data Integrity Failures
python3 tests/security/test_input_validation.py
python3 tests/security/test_deserialization.py

# A09: Logging/Monitoring Failures
python3 tests/security/test_audit_logs.py

# A10: SSRF
python3 tests/security/test_ssrf.py
```

### 3. Penetration Testing (Simulated Attacks)
**Attack Scenarios**:
- **JWT Token Manipulation**: Attempt to forge/tamper with JWT signatures
- **SQL Injection**: Test all database query endpoints with payloads
- **XSS**: Inject scripts in all user input fields (search, forms, URLs)
- **CSRF**: Test state-changing operations without CSRF tokens
- **Session Hijacking**: Attempt to steal/reuse session tokens
- **API Rate Limit Bypass**: Try to circumvent rate limiting
- **Mass Assignment**: Attempt to modify unauthorized fields via API
- **Path Traversal**: Test file upload/download for directory traversal
- **Business Logic Flaws**: Test discount stacking, negative quantities, price manipulation

### 4. Secrets & Credentials Security
**Checks**:
- No hardcoded passwords, API keys, or tokens in code/configs
- All secrets stored in Google Secret Manager (not env vars)
- Secrets rotation policy enforced (90 days max)
- Database credentials use IAM authentication (no passwords)
- API keys have proper scopes and expiration
- Service account keys audited (none should exist for long-term use)

### 5. Network & Infrastructure Security
**Cloud Security Validation**:
- VPC firewall rules (only necessary ports open)
- IAM roles follow least privilege (no overly permissive roles)
- Cloud Storage buckets not public (unless intentional)
- Cloud SQL instances not publicly accessible
- SSL/TLS certificate validity (no self-signed in production)
- HTTPS enforced (no HTTP endpoints)
- Security groups properly configured

### 6. Data Protection & Privacy
**Compliance Testing**:
- PII data encrypted at rest (database encryption enabled)
- PII data encrypted in transit (TLS 1.2+)
- Data masking in logs (no PII in application logs)
- GDPR compliance (data deletion, export capabilities)
- Payment data never stored (tokenization validated)
- Audit logs capture all sensitive data access

### 7. Vulnerability Management
**CVE Tracking & Patching**:
- Weekly dependency scans (Python, JavaScript, Docker images)
- Critical/High CVEs remediated within 7 days
- Medium CVEs remediated within 30 days
- Low CVEs addressed in next sprint
- Security advisory subscriptions (GitHub, Snyk, Google Cloud)

---

## âš¡ PERFORMANCE TESTING CAPABILITIES (Performance Agent Expertise)

### 1. Performance Architecture Review
**Triggered**: When Architect completes design

**Activities**:
- Review performance requirements (latency targets, throughput, concurrent users)
- Identify bottlenecks (database queries, API calls, CPU-intensive operations)
- Assess caching strategy (Redis, CDN, browser caching)
- Review database indexing strategy
- Validate async/background job usage (Celery tasks)
- Check for N+1 query problems

### 2. Load Testing (Simulated Production Traffic)
**Tools**: Locust, Artillery, k6

**Test Scenarios**:
```python
# locustfile.py - Simulates 1000 concurrent users
from locust import HttpUser, task, between

class AgentMarketplaceUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(3)  # 60% of traffic
    def browse_agents(self):
        self.client.get("/api/agents?industry=marketing&limit=20")
    
    @task(2)  # 40% of traffic
    def view_agent_details(self):
        agent_id = random.randint(1, 19)
        self.client.get(f"/api/agents/{agent_id}")
    
    @task(1)  # 20% of traffic
    def start_trial(self):
        self.client.post("/api/trials", json={"agent_id": 1, "duration": 7})
```

**Performance Targets**:
- **API Response Time**: P95 < 200ms, P99 < 500ms
- **Page Load Time**: P95 < 2s, P99 < 3s
- **Throughput**: 1000 req/sec sustained
- **Error Rate**: < 0.1% under load
- **Database Query Time**: P95 < 50ms

### 3. Stress Testing (Breaking Point Analysis)
**Goal**: Find system limits

**Process**:
- Gradually increase load (100 â†’ 500 â†’ 1000 â†’ 2000 users)
- Identify breaking point (when error rate > 1% or latency > 2x normal)
- Monitor resource utilization (CPU, memory, database connections)
- Validate auto-scaling triggers (Cloud Run scales at 80% CPU)
- Verify graceful degradation (system slows but doesn't crash)

### 4. Endurance Testing (Stability Over Time)
**Goal**: Detect memory leaks, resource exhaustion

**Test Plan**:
- Run constant load (500 concurrent users) for 24 hours
- Monitor memory usage trend (should be flat, not increasing)
- Check for file descriptor leaks (socket connections)
- Validate database connection pool stability
- Monitor error rate over time (should remain < 0.1%)

### 5. Database Performance Optimization
**Query Analysis**:
```sql
-- Identify slow queries (> 100ms)
SELECT query, mean_exec_time, calls
FROM pg_stat_statements
WHERE mean_exec_time > 100
ORDER BY mean_exec_time DESC
LIMIT 20;

-- Check missing indexes
SELECT schemaname, tablename, attname, n_distinct
FROM pg_stats
WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
  AND n_distinct > 100
  AND attname NOT IN (SELECT a.attname FROM pg_index i
                      JOIN pg_attribute a ON a.attnum = ANY(i.indkey)
                      WHERE i.indrelid = pg_stats.tablename::regclass);
```

**Optimization Actions**:
- Add indexes on frequently queried columns (industry, rating, status)
- Use composite indexes for multi-column filters
- Implement query result caching (Redis)
- Use database connection pooling (pgbouncer)
- Archive old data (partitioning for time-series data)

### 6. API Performance Profiling
**Tools**: py-spy, cProfile, Django Silk

**Profiling Process**:
```bash
# Profile Python backend
py-spy record -o profile.svg --pid $(pgrep -f "uvicorn")

# Identify slow functions
python3 -m cProfile -o api_profile.stats tests/load/run_api_load.py
python3 -m pstats api_profile.stats
```

**Optimization Targets**:
- Reduce function call overhead (eliminate unnecessary loops)
- Optimize serialization (use orjson instead of json)
- Minimize database round trips (use select_related, prefetch_related)
- Implement pagination (avoid SELECT * queries)
- Use async I/O for external API calls

### 7. Frontend Performance
**Metrics** (via Lighthouse, WebPageTest):
- **First Contentful Paint**: < 1.5s
- **Largest Contentful Paint**: < 2.5s
- **Time to Interactive**: < 3.5s
- **Cumulative Layout Shift**: < 0.1
- **Total Blocking Time**: < 200ms

**Optimizations**:
- Lazy load images (loading="lazy")
- Code splitting (dynamic imports)
- Bundle size optimization (tree shaking, minification)
- CDN for static assets (Cloud CDN)
- Service worker caching (PWA capabilities)

### 8. Capacity Planning
**Forecasting Resource Needs**:
- Current capacity: 1000 concurrent users
- Growth projection: 30% month-over-month
- Forecast: 4000 concurrent users in 6 months
- Required scaling: 4x Cloud Run instances, 2x database CPU
- Cost projection: â‚¹15,000/month â†’ â‚¹45,000/month
- Recommendation: Implement caching to reduce by 50%

---

## ðŸ§ª CORE TESTING ENHANCEMENTS (Addressing Existing Gaps)

### 1. Accessibility Testing (WCAG 2.1 AA Compliance)
**Tools**: axe-core, Pa11y, Lighthouse

**Test Coverage**:
- Keyboard navigation (all features accessible without mouse)
- Screen reader compatibility (ARIA labels, semantic HTML)
- Color contrast (4.5:1 minimum for text)
- Focus indicators (visible focus rings)
- Form labels and error messages (associated with inputs)
- Alternative text for images
- Captions for videos

**Automated Tests**:
```javascript
// tests/accessibility/test_a11y.spec.js
test('Agent marketplace page is accessible', async ({ page }) => {
  await page.goto('https://demo.waooaw.com/agents');
  const accessibilityScanResults = await new AxePuppeteer(page).analyze();
  expect(accessibilityScanResults.violations).toHaveLength(0);
});
```

### 2. Regression Test Suite Management
**Strategy**:
- Maintain golden dataset for regression tests (500 test cases)
- Categorize tests: P0 (critical, 50 tests), P1 (high, 150 tests), P2 (medium, 300 tests)
- Run P0 tests on every PR (< 5 min execution time)
- Run full suite nightly (< 30 min execution time)
- Flaky test detection (tests that fail inconsistently)
- Automatic test quarantine (flaky tests moved to separate suite)

**Flaky Test Handling**:
```yaml
# .github/workflows/test-regression.yml
- name: Run regression tests with retry
  run: pytest tests/regression/ --reruns 3 --reruns-delay 2
  
- name: Detect flaky tests
  run: python3 scripts/detect_flaky_tests.py --failures=3 --quarantine
```

### 3. Test Data Management
**Synthetic Data Generation**:
```python
# tests/fixtures/data_factory.py
import factory
from faker import Faker

fake = Faker()

class AgentFactory(factory.Factory):
    class Meta:
        model = Agent
    
    name = factory.LazyAttribute(lambda _: f"{fake.job()} Agent")
    industry = factory.Iterator(['marketing', 'education', 'sales'])
    specialty = factory.LazyAttribute(lambda o: f"{o.industry} specialist")
    rating = factory.LazyAttribute(lambda _: round(random.uniform(3.5, 5.0), 1))
    price = factory.LazyAttribute(lambda _: random.randint(8000, 18000))
```

**Test Data Lifecycle**:
- Generate fresh test data before each test run
- Anonymize production data for testing (mask PII)
- Clean up test data after test execution
- Maintain test data versioning (snapshots for regression tests)

### 4. Production Monitoring Validation
**Observability Tests**:
```python
# tests/observability/test_monitoring.py
def test_logs_contain_required_fields():
    """Verify structured logging includes trace_id, user_id, request_id"""
    log_entry = simulate_api_request()
    assert 'trace_id' in log_entry
    assert 'user_id' in log_entry
    assert 'request_id' in log_entry
    assert 'latency_ms' in log_entry

def test_metrics_exported():
    """Verify Prometheus metrics are exported"""
    response = requests.get('http://localhost:8000/metrics')
    assert 'http_requests_total' in response.text
    assert 'http_request_duration_seconds' in response.text

def test_distributed_tracing():
    """Verify OpenTelemetry traces propagate across services"""
    trace = make_api_call_with_trace_id()
    assert trace.spans includes ['gateway', 'backend', 'database']
```

**SLO Validation**:
- Availability: 99.9% (< 43 min downtime/month)
- Latency: P95 < 200ms
- Error Rate: < 0.1%
- Monitor SLO burn rate (alert if trending towards budget exhaustion)

---

## ðŸ“Š TESTING METRICS & REPORTING

### Enhanced Coverage Tracking
- **Unit Test Coverage**: 85% minimum (Coding Agent writes these)
- **Integration Test Coverage**: 70% minimum (Testing Agent writes these)
- **E2E Test Coverage**: 90% of critical user journeys
- **Security Test Coverage**: 100% of OWASP Top 10
- **Performance Test Coverage**: All APIs under load
- **Accessibility Test Coverage**: All user-facing pages

### Test Execution Dashboard
**Metrics Tracked**:
- Total tests: 1,247
- Passing: 1,242 (99.6%)
- Failing: 3 (0.2%)
- Flaky: 2 (0.2%)
- Execution time: 23 minutes
- Code coverage: 87%
- Security vulnerabilities: 0 critical, 2 medium
- Performance: P95 latency 185ms âœ… (target < 200ms)
- Accessibility: 0 violations âœ…

---

## ðŸ”„ TESTING WORKFLOW INTEGRATION

### Trigger Points
1. **On PR Creation**: Run unit + integration tests (< 10 min)
2. **On PR Merge**: Run full regression suite (< 30 min)
3. **Nightly**: Run security + performance + accessibility tests
4. **Pre-Deployment**: Run smoke tests + security scans
5. **Post-Deployment**: Run production smoke tests + validate SLOs

### Handover to Deployment Agent
**Final Validation Checklist**:
- âœ… All unit tests passing (85%+ coverage)
- âœ… All integration tests passing
- âœ… E2E tests passing (critical journeys)
- âœ… Security scan: 0 critical/high vulnerabilities
- âœ… Performance: P95 latency < 200ms
- âœ… Accessibility: WCAG 2.1 AA compliant
- âœ… Regression tests passing
- âœ… Test data cleaned up

**Issue Creation**: `testing-complete` label added to epic
**Notification**: `@DeploymentAgent infrastructure code generation ready`

---

## ðŸŽ¯ SUCCESS METRICS

### Security
- Zero critical vulnerabilities in production
- 100% OWASP Top 10 coverage
- <7 days to patch high-severity CVEs
- Zero data breaches
- 100% secrets in Secret Manager (no hardcoded)

### Performance
- P95 API latency < 200ms
- P95 page load < 2s
- 99.9% availability (< 43 min downtime/month)
- Auto-scaling working (no manual intervention)
- Zero out-of-memory errors

### Quality
- 87%+ code coverage
- <1% flaky test rate
- <0.1% error rate in production
- WCAG 2.1 AA compliance (100% pages)
- Zero production-breaking bugs per release

---

**Next**: See [Coding Agent Enhanced](coding_agent_enhanced_capabilities.md) for Data Agent expertise
**Next**: See [Deployment Agent Enhanced](deployment_agent_enhanced_capabilities.md) for DevOps/SRE expertise
