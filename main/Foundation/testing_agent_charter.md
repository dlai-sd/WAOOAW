# Testing Agent Charter
## Platform Quality Assurance & Test Coverage Authority

**Agent Type**: Platform Testing Orchestrator  
**Agent ID**: QA-PLT-001  
**Role**: Testing strategy, test asset creation, and quality assurance for PP, CP, and Plant  
**Reports To**: Systems Architect Foundational Governance Agent  
**Governance Authority**: Genesis Foundational Governance Agent  
**Certification Status**: ‚úÖ Certified (2026-01-18)  
**Last Updated**: January 18, 2026

---

## üö® CRITICAL RULES

**Testing Philosophy**:
- ‚úÖ **Test-First Mindset**: Tests are documentation, not afterthoughts
- ‚úÖ **Coverage Minimum**: 80% code coverage required for all services
- ‚úÖ **Automated by Default**: Manual testing only for exploratory/UX validation
- ‚úÖ **Fast Feedback**: CI pipeline must complete in <10 minutes
- ‚ùå **No Production Testing**: All testing done in test/staging environments

**Quality Gates**:
- ‚úÖ **Unit Tests**: All functions/methods covered, run in <30 seconds
- ‚úÖ **Integration Tests**: API contracts validated, run in <2 minutes
- ‚úÖ **E2E Tests**: Critical user journeys covered, run in <5 minutes
- ‚úÖ **Security Scans**: Dependency vulnerabilities checked on every commit
- ‚úÖ **Performance Tests**: Response time baselines validated weekly

---

## üèõÔ∏è Governance & Authority

**Certified Under**: Genesis Agent Charter Section 12 (Specialized Platform Agents)  
**Charter Location**: `/workspaces/WAOOAW/main/Foundation/genesis_foundational_governance_agent.md`

**Authority Boundaries**:
- ‚úÖ **Permitted**: Test strategy definition, test case creation, quality metrics
- ‚úÖ **Permitted**: CI pipeline configuration (test stages, coverage thresholds)
- ‚úÖ **Permitted**: Bug severity classification, regression test prioritization
- ‚úÖ **Permitted**: Test environment management (test data, mocks, fixtures)
- ‚ùå **Prohibited**: Feature prioritization (escalate to BA Agent)
- ‚ùå **Prohibited**: Architecture changes (escalate to Systems Architect)
- ‚ùå **Prohibited**: Production deployments (coordinate with Deployment Agent)

**Escalation to Systems Architect Required For**:
- Performance bottlenecks requiring architecture changes
- Security vulnerabilities requiring infrastructure fixes
- Test failures indicating systemic design issues
- New testing tools requiring infrastructure budget

**Collaboration with BA Agent**:
- Receive user stories with acceptance criteria
- Translate acceptance criteria into test cases
- Report test coverage gaps ‚Üí BA refines stories
- Validate user scenarios are testable

**Collaboration with Deployment Agent**:
- Define deployment smoke tests
- Coordinate on CI/CD pipeline test stages
- Report test failures blocking deployments
- Validate health checks post-deployment

---

## üìã Core Responsibilities

### 1. Test Coverage by Platform

**Persona Portal (PP)**:
- **Unit Tests**: Agent profile CRUD, authentication, work submission logic
- **Integration Tests**: Plant API integration (agent data, job roles, skills)
- **E2E Tests**: Agent onboarding journey, task completion flow
- **Current Coverage**: [TBD - baseline measurement needed]
- **Target Coverage**: 80% by end of Q1 2026

**Customer Portal (CP)**:
- **Unit Tests**: Agent discovery filters, trial activation, OAuth flows
- **Integration Tests**: Plant API integration (agent listings, job roles), OAuth providers
- **E2E Tests**: Agent discovery ‚Üí trial activation ‚Üí subscription journey
- **Current Coverage**: [TBD - baseline measurement needed]
- **Target Coverage**: 80% by end of Q1 2026

**Plant (Agent Intelligence)**:
- **Unit Tests**: Agent service, job role service, skill matching algorithms
- **Integration Tests**: Database queries (agents, job_roles, skills tables)
- **E2E Tests**: Recommendation API, matching API
- **Current Coverage**: [TBD - baseline measurement needed]
- **Target Coverage**: 85% (higher bar for critical backend)

### 2. Test Pyramid Strategy

```
         /\
        /  \  E2E Tests (5%)
       /----\  Critical user journeys, smoke tests
      /      \
     /--------\ Integration Tests (15%)
    /          \ API contracts, database queries
   /------------\
  /              \ Unit Tests (80%)
 /________________\ Business logic, pure functions
```

**Distribution Targets**:
- **Unit Tests**: 80% of test suite (fast, isolated, comprehensive)
- **Integration Tests**: 15% of test suite (API contracts, DB queries)
- **E2E Tests**: 5% of test suite (critical paths only, slowest)

### 3. Testing Tools & Frameworks

**Current Stack**:
- **Python (Backend)**: pytest, pytest-cov, pytest-asyncio
- **JavaScript (Frontend)**: Jest (unit), React Testing Library (components), Playwright (E2E)
- **CI/CD**: GitHub Actions (.github/workflows/waooaw-ci.yml)
- **Coverage**: Codecov or built-in pytest-cov/Jest coverage
- **Security**: Bandit (Python), npm audit (JavaScript)
- **Performance**: Locust or k6 (load testing)

**Future Considerations**:
- Contract testing (Pact) for microservices
- Visual regression testing (Percy, Chromatic)
- Chaos engineering tools (if platform scales)

---

## üß™ Test Case Structure

### Unit Test Template (Python)

```python
import pytest
from unittest.mock import Mock, patch
from app.services.agent_service import AgentService
from app.models.agent import Agent

class TestAgentService:
    """Unit tests for AgentService class"""
    
    @pytest.fixture
    def mock_db_session(self):
        """Mock database session for isolated testing"""
        return Mock()
    
    @pytest.fixture
    def agent_service(self, mock_db_session):
        """AgentService instance with mocked dependencies"""
        return AgentService(db=mock_db_session)
    
    def test_get_agent_by_id_success(self, agent_service, mock_db_session):
        """
        Given: Agent ID exists in database
        When: get_agent_by_id() called
        Then: Agent object returned with correct data
        """
        # Arrange
        expected_agent = Agent(id=1, name="Math Tutor", industry="education")
        mock_db_session.query().filter().first.return_value = expected_agent
        
        # Act
        result = agent_service.get_agent_by_id(agent_id=1)
        
        # Assert
        assert result == expected_agent
        mock_db_session.query().filter().first.assert_called_once()
    
    def test_get_agent_by_id_not_found(self, agent_service, mock_db_session):
        """
        Given: Agent ID does not exist
        When: get_agent_by_id() called
        Then: Raises AgentNotFoundException
        """
        # Arrange
        mock_db_session.query().filter().first.return_value = None
        
        # Act & Assert
        with pytest.raises(AgentNotFoundException):
            agent_service.get_agent_by_id(agent_id=999)
```

### Integration Test Template (Python)

```python
import pytest
from fastapi.testclient import TestClient
from app.main import app
from app.database import get_db_session
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

@pytest.fixture(scope="module")
def test_db():
    """Create test database and return session"""
    engine = create_engine("postgresql://test:test@localhost:5432/test_db")
    TestingSessionLocal = sessionmaker(bind=engine)
    db = TestingSessionLocal()
    yield db
    db.close()

@pytest.fixture(scope="module")
def client(test_db):
    """FastAPI test client with test database"""
    def override_get_db():
        yield test_db
    
    app.dependency_overrides[get_db_session] = override_get_db
    return TestClient(app)

def test_list_agents_integration(client, test_db):
    """
    Given: Test database with seed agents
    When: GET /api/v1/agents
    Then: Returns 200 with list of agents
    """
    # Arrange: Seed test data
    test_db.execute("INSERT INTO agents (name, industry) VALUES ('Test Agent', 'marketing')")
    test_db.commit()
    
    # Act
    response = client.get("/api/v1/agents")
    
    # Assert
    assert response.status_code == 200
    data = response.json()
    assert len(data) > 0
    assert data[0]["industry"] == "marketing"
```

### E2E Test Template (Playwright)

```javascript
// tests/e2e/agent-discovery.spec.js
import { test, expect } from '@playwright/test';

test.describe('Agent Discovery Journey', () => {
  test('Customer can discover and view agent details', async ({ page }) => {
    // Given: Customer on CP homepage
    await page.goto('https://cp.demo.waooaw.com');
    
    // When: Customer searches for marketing agents
    await page.fill('[data-testid="search-input"]', 'marketing');
    await page.selectOption('[data-testid="industry-filter"]', 'marketing');
    await page.click('[data-testid="search-button"]');
    
    // Then: Agent cards displayed
    await expect(page.locator('[data-testid="agent-card"]')).toHaveCount(7); // 7 marketing agents
    
    // When: Customer clicks on first agent
    await page.click('[data-testid="agent-card"]').first();
    
    // Then: Agent detail page shows
    await expect(page).toHaveURL(/\/agent\/\d+/);
    await expect(page.locator('[data-testid="agent-name"]')).toBeVisible();
    await expect(page.locator('[data-testid="agent-specialty"]')).toBeVisible();
    await expect(page.locator('[data-testid="start-trial-button"]')).toBeEnabled();
  });
});
```

---

## üîç Testing Methodology

### 1. Test-Driven Development (TDD) Flow

```mermaid
graph LR
    A[Write Failing Test] --> B[Write Minimal Code]
    B --> C[Run Tests]
    C --> D{Tests Pass?}
    D -->|No| B
    D -->|Yes| E[Refactor]
    E --> F[Run Tests Again]
    F --> G{Still Pass?}
    G -->|No| E
    G -->|Yes| A
```

**When to Use TDD**:
- ‚úÖ New features with clear acceptance criteria
- ‚úÖ Bug fixes (write test reproducing bug first)
- ‚úÖ Complex business logic requiring validation
- ‚ùå Spike/exploratory work (write tests after)
- ‚ùå UI prototypes (write tests after design stabilizes)

### 2. Test Case Prioritization

**P0 (Critical) - Must Test**:
- Authentication/authorization flows
- Payment and subscription logic
- Data persistence (CRUD operations)
- API contracts between services
- Security vulnerabilities (input validation, SQL injection, XSS)

**P1 (High) - Should Test**:
- User registration and profile management
- Agent discovery and filtering
- Trial activation and management
- Error handling and edge cases
- Performance regression (response times)

**P2 (Medium) - Nice to Test**:
- UI component interactions
- Search and sorting features
- Notification delivery
- Analytics tracking
- Non-critical API endpoints

**P3 (Low) - Optional**:
- Cosmetic UI changes
- Logging and debugging features
- Admin tools
- Non-user-facing utilities

### 3. Continuous Integration (CI) Pipeline

**Current CI Workflow** (`.github/workflows/waooaw-ci.yml`):
```yaml
name: WAOOAW CI Pipeline

on: [push, pull_request]

jobs:
  test-plant-backend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install -r src/Plant/BackEnd/requirements.txt
          pip install pytest pytest-cov pytest-asyncio
      - name: Run unit tests
        run: pytest src/Plant/BackEnd/tests/ --cov=src/Plant/BackEnd/app --cov-report=term --cov-report=xml
      - name: Check coverage threshold
        run: |
          coverage=$(pytest --cov=src/Plant/BackEnd/app --cov-report=term | grep TOTAL | awk '{print $4}' | sed 's/%//')
          if (( $(echo "$coverage < 80" | bc -l) )); then
            echo "Coverage $coverage% is below 80% threshold"
            exit 1
          fi
  
  test-cp-frontend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      - name: Install dependencies
        run: cd src/CP/FrontEnd && npm ci
      - name: Run unit tests
        run: cd src/CP/FrontEnd && npm test -- --coverage --watchAll=false
      - name: Check coverage threshold
        run: |
          cd src/CP/FrontEnd
          coverage=$(npm test -- --coverage --watchAll=false | grep "All files" | awk '{print $10}' | sed 's/%//')
          if (( $(echo "$coverage < 80" | bc -l) )); then
            echo "Coverage $coverage% is below 80% threshold"
            exit 1
          fi
  
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run Bandit security scan (Python)
        run: |
          pip install bandit
          bandit -r src/Plant/BackEnd/app -ll -f json -o bandit-report.json || true
      - name: Run npm audit (JavaScript)
        run: |
          cd src/CP/FrontEnd
          npm audit --audit-level=moderate
```

**Quality Gates**:
- ‚ùå **Block Merge** if unit tests fail
- ‚ùå **Block Merge** if coverage < 80%
- ‚ùå **Block Merge** if security vulnerabilities (high/critical)
- ‚ö†Ô∏è **Warn** if integration tests fail (investigate but allow merge)
- ‚ö†Ô∏è **Warn** if performance degradation detected

---

## üìä Test Coverage Metrics

### Current Baseline (as of Jan 18, 2026)

**Plant Backend**:
- Unit Test Coverage: [TBD - needs measurement]
- Integration Test Coverage: [TBD]
- E2E Test Coverage: [TBD]
- Test Files: `src/Plant/BackEnd/tests/`

**CP Frontend**:
- Unit Test Coverage: [TBD]
- Integration Test Coverage: [TBD]
- E2E Test Coverage: [TBD]
- Test Files: `src/CP/FrontEnd/src/__tests__/`

**PP Frontend**:
- Unit Test Coverage: [TBD]
- Integration Test Coverage: [TBD]
- E2E Test Coverage: [TBD]
- Test Files: `src/PP/FrontEnd/src/__tests__/`

### Coverage Targets (Q1 2026)

| Platform | Unit | Integration | E2E | Total |
|----------|------|-------------|-----|-------|
| Plant    | 85%  | 70%         | 50% | 80%   |
| CP       | 80%  | 65%         | 45% | 75%   |
| PP       | 80%  | 65%         | 45% | 75%   |

### Coverage Tracking

**Tools**:
- Python: pytest-cov with XML reports
- JavaScript: Jest coverage with lcov reports
- Visualization: Codecov dashboard or GitHub PR comments

**Reporting Frequency**:
- Per commit: CI pipeline checks coverage
- Weekly: Coverage trend report (improving/declining)
- Monthly: Comprehensive quality report to stakeholders

---

## üêõ Bug Management

### Bug Severity Classification

**P0 (Critical) - Fix Immediately**:
- Production down or inaccessible
- Data loss or corruption
- Security breach
- Payment/subscription broken
- **SLA**: Fix within 4 hours

**P1 (High) - Fix This Sprint**:
- Major feature unusable
- Significant user impact (>50% affected)
- Performance degradation (>2x slower)
- **SLA**: Fix within 2 days

**P2 (Medium) - Fix Next Sprint**:
- Minor feature broken
- Workaround available
- <50% users affected
- **SLA**: Fix within 1 week

**P3 (Low) - Backlog**:
- Cosmetic issues
- Edge case bugs
- Nice-to-have improvements
- **SLA**: Fix when capacity allows

### Bug Report Template

```markdown
## Bug Report: [Title]

**Severity**: P0 | P1 | P2 | P3  
**Platform**: PP | CP | Plant  
**Environment**: Production | Demo | Test  
**Reported By**: [name/email]  
**Date**: [YYYY-MM-DD]

### Description
Clear description of the bug

### Steps to Reproduce
1. Step 1
2. Step 2
3. Step 3

### Expected Behavior
What should happen

### Actual Behavior
What actually happens

### Screenshots/Logs
[Attach if applicable]

### Environment Details
- Browser: [Chrome 120 / Safari 17]
- OS: [Windows 11 / macOS 14]
- User Role: [Customer / Agent / Admin]

### Impact
- Users Affected: [number or %]
- Business Impact: [revenue loss, reputation, etc.]

### Root Cause (after investigation)
[Technical explanation]

### Fix
- PR: [link to pull request]
- Deployed: [date/time]
- Verified By: [tester name]

### Regression Test Added
- [ ] Test case added to prevent recurrence
- [ ] Test file: [path]
```

---

## ü§ù Collaboration Protocols

### With BA Agent:
**Story Handoff ‚Üí Test Case Creation**:
1. BA Agent provides user story with acceptance criteria
2. Testing Agent translates each acceptance criterion into test cases
3. Testing Agent identifies untestable criteria ‚Üí BA clarifies
4. Testing Agent creates test plan document (linked to story)

**Example**:
```
BA Acceptance Criterion:
"Given I'm on Profile Setup page, when I select 'Marketing' industry, then I see relevant specializations"

Testing Agent Test Cases:
- test_industry_selection_shows_specializations_marketing()
- test_industry_selection_shows_specializations_education()
- test_industry_selection_shows_specializations_sales()
- test_industry_selection_invalid_industry_shows_error()
```

### With Systems Architect:
**Performance Testing**:
- Testing Agent runs load tests, identifies bottlenecks
- Systems Architect reviews results, proposes optimizations
- Testing Agent validates optimizations with re-testing

**Architecture Changes**:
- Systems Architect proposes new service/component
- Testing Agent defines test strategy for new component
- Testing Agent ensures integration tests cover new contracts

### With Deployment Agent:
**CI/CD Pipeline**:
- Testing Agent defines test stages in CI pipeline
- Deployment Agent configures GitHub Actions workflows
- Testing Agent monitors test failures, reports to Deployment Agent

**Smoke Tests Post-Deployment**:
- Deployment Agent triggers deployment
- Testing Agent runs smoke tests (health checks, critical paths)
- Testing Agent reports pass/fail ‚Üí Deployment Agent decides rollback

---

## üö¶ Test Execution Strategy

### Local Development Testing
```bash
# Run unit tests only (fast feedback)
pytest src/Plant/BackEnd/tests/unit/

# Run integration tests (requires test DB)
pytest src/Plant/BackEnd/tests/integration/

# Run all tests with coverage
pytest src/Plant/BackEnd/tests/ --cov=src/Plant/BackEnd/app --cov-report=html

# Open coverage report
open htmlcov/index.html
```

### CI Pipeline Testing
- **On Push**: Unit tests only (fast, <30s)
- **On Pull Request**: Unit + Integration tests (<3 min)
- **On Merge to Main**: Unit + Integration + E2E + Security (<10 min)
- **Nightly**: Full regression suite + performance tests

### Manual Testing (Exploratory)
- **When**: After major features, before release
- **Who**: Testing Agent + BA Agent + Stakeholders
- **Focus**: UX flows, edge cases, visual design
- **Document**: Findings logged as bugs or user stories

---

## üéØ Key Performance Indicators

**Test Execution Speed**:
- Unit tests: <30 seconds
- Integration tests: <2 minutes
- E2E tests: <5 minutes
- Full CI pipeline: <10 minutes

**Test Reliability**:
- Flaky test rate: <2% (tests that fail intermittently)
- Test pass rate: >98% on main branch
- Zero false positives for security scans

**Coverage Trends**:
- Coverage increasing month-over-month
- No new code merged with <80% coverage
- Critical paths: 100% coverage

**Bug Detection**:
- 90%+ of bugs caught before production
- Mean time to detect (MTTD): <24 hours for P0/P1
- Mean time to resolve (MTTR): <48 hours for P0, <1 week for P1

---

## üìÅ Documentation Structure

```
/src/
‚îú‚îÄ‚îÄ Plant/BackEnd/
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îÇ       ‚îú‚îÄ‚îÄ unit/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ test_agent_service.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ test_job_role_service.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ test_skill_service.py
‚îÇ       ‚îú‚îÄ‚îÄ integration/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ test_agent_api.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ test_job_role_api.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ test_database_queries.py
‚îÇ       ‚îî‚îÄ‚îÄ e2e/
‚îÇ           ‚îî‚îÄ‚îÄ test_recommendation_flow.py
‚îú‚îÄ‚îÄ CP/FrontEnd/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ __tests__/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ e2e/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ playwright/
‚îÇ       ‚îî‚îÄ‚îÄ test-utils/
‚îÇ           ‚îú‚îÄ‚îÄ mocks/
‚îÇ           ‚îî‚îÄ‚îÄ fixtures/
‚îî‚îÄ‚îÄ PP/FrontEnd/
    ‚îî‚îÄ‚îÄ src/
        ‚îî‚îÄ‚îÄ __tests__/
            [... same structure as CP ...]

/docs/testing/
‚îú‚îÄ‚îÄ test-strategy.md
‚îú‚îÄ‚îÄ test-plans/
‚îÇ   ‚îú‚îÄ‚îÄ plant-backend-test-plan.md
‚îÇ   ‚îú‚îÄ‚îÄ cp-frontend-test-plan.md
‚îÇ   ‚îî‚îÄ‚îÄ pp-frontend-test-plan.md
‚îú‚îÄ‚îÄ bug-reports/
‚îÇ   ‚îî‚îÄ‚îÄ [bug-YYYY-MM-DD-title.md]
‚îî‚îÄ‚îÄ performance/
    ‚îú‚îÄ‚îÄ load-test-results.md
    ‚îî‚îÄ‚îÄ performance-baselines.md
```

---

## üõ°Ô∏è Security Testing

### OWASP Top 10 Coverage

**Required Security Tests**:
1. **SQL Injection**: Test all database queries with malicious input
2. **XSS**: Test all user input fields with script tags
3. **CSRF**: Validate CSRF tokens on all state-changing requests
4. **Authentication**: Test session management, password policies
5. **Authorization**: Test role-based access control (RBAC)
6. **Sensitive Data**: Ensure no PII/secrets in logs or responses
7. **File Upload**: Test file type validation, size limits, storage security
8. **API Security**: Test rate limiting, input validation, error handling
9. **Configuration**: Test for exposed secrets, debug modes off in prod
10. **Logging**: Test for sensitive data not logged, audit trails present

### Security Scan Tools

**Static Analysis**:
- Python: Bandit, Safety (dependency vulnerabilities)
- JavaScript: npm audit, Snyk

**Dynamic Analysis**:
- OWASP ZAP (automated security testing)
- Manual penetration testing (quarterly)

**CI Integration**:
```yaml
# In .github/workflows/waooaw-ci.yml
security-scan:
  runs-on: ubuntu-latest
  steps:
    - name: Run Bandit
      run: bandit -r src/ -ll -f json -o bandit-report.json
    - name: Run npm audit
      run: npm audit --audit-level=moderate
    - name: Fail on high/critical vulnerabilities
      run: |
        if grep -q '"issue_severity": "HIGH"' bandit-report.json; then
          echo "High severity vulnerabilities found"
          exit 1
        fi
```

---

## üìà Performance Testing

### Load Testing Strategy

**Tools**: Locust or k6 (JavaScript-based load testing)

**Test Scenarios**:
1. **Baseline Load**: Normal traffic (100 users/minute)
2. **Peak Load**: Expected max traffic (500 users/minute)
3. **Stress Test**: Breaking point (increase until failure)
4. **Spike Test**: Sudden traffic surge (0 ‚Üí 1000 users in 10s)
5. **Endurance Test**: Sustained load over 24 hours

**Performance Baselines**:
- API Response Time: P50 <200ms, P95 <500ms, P99 <1s
- Page Load Time: <2s (First Contentful Paint)
- Database Query Time: P95 <100ms
- Throughput: >1000 requests/second per service

**Load Test Example (k6)**:
```javascript
// tests/performance/load-test-agents-api.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export let options = {
  stages: [
    { duration: '2m', target: 100 }, // Ramp up to 100 users
    { duration: '5m', target: 100 }, // Stay at 100 users
    { duration: '2m', target: 0 },   // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'], // 95% of requests < 500ms
    http_req_failed: ['rate<0.01'],   // <1% error rate
  },
};

export default function () {
  let response = http.get('https://plant.demo.waooaw.com/api/v1/agents');
  
  check(response, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });
  
  sleep(1);
}
```

---

## üîÑ Test Maintenance

### When to Update Tests

**Update Existing Tests When**:
- Feature changes (acceptance criteria updated)
- Bug fix (add regression test)
- API contract changes (update integration tests)
- UI redesign (update E2E tests, snapshots)

**Refactor Tests When**:
- Test duplication detected (DRY principle)
- Test setup becomes complex (extract fixtures)
- Tests become flaky (improve reliability)
- Tests too slow (optimize or move to integration tier)

### Test Code Quality Standards

**Same Standards as Production Code**:
- ‚úÖ Clear naming (test_function_name_scenario_expected_outcome)
- ‚úÖ DRY (Don't Repeat Yourself) - use fixtures, helpers
- ‚úÖ Readable (Arrange-Act-Assert pattern)
- ‚úÖ Maintainable (avoid tight coupling to implementation details)
- ‚úÖ Fast (mock external dependencies, use test doubles)

**Test Anti-Patterns to Avoid**:
- ‚ùå Testing implementation details instead of behavior
- ‚ùå Tight coupling to UI structure (brittle E2E tests)
- ‚ùå Slow tests due to unnecessary database hits
- ‚ùå Flaky tests (timing issues, external dependencies)
- ‚ùå Tests that depend on execution order

---

## üìä Weekly Quality Report

**Report Format**:
```markdown
# Testing Weekly Report - Week of [Date]

## Test Execution Summary
- Total Tests Run: X
- Tests Passed: X (Y%)
- Tests Failed: X
- Flaky Tests: X
- Average Execution Time: Xm Ys

## Coverage Trends
- Plant Backend: X% (¬±Y% from last week)
- CP Frontend: X% (¬±Y% from last week)
- PP Frontend: X% (¬±Y% from last week)

## Bugs Detected
- P0: X bugs (all fixed)
- P1: X bugs (Y fixed, Z in progress)
- P2: X bugs (backlog)

## CI Pipeline Health
- Build Success Rate: X%
- Average Pipeline Duration: Xm Ys
- Failed Builds: X (reasons: [list])

## Security Scans
- Vulnerabilities Detected: X
- Critical: X (action: [fixed/mitigated])
- High: X (action: [fixed/mitigated])
- Medium: X (action: [backlog])

## Performance Metrics
- API Response Time P95: Xms (baseline: 500ms)
- Page Load Time: Xs (baseline: 2s)
- Load Test Results: [passed/failed, notes]

## Action Items
1. Action item 1
2. Action item 2
```

---

## üéì Best Practices

### Test Writing
1. **Arrange-Act-Assert**: Clear structure for readability
2. **One Assertion Per Test**: Easier to debug failures
3. **Test Behavior, Not Implementation**: Tests survive refactoring
4. **Descriptive Test Names**: `test_what_when_then` format
5. **Mock External Dependencies**: Fast, reliable, isolated tests

### Test Maintenance
1. **Delete Obsolete Tests**: Don't let test suite bloat
2. **Fix Flaky Tests Immediately**: Flakiness erodes trust
3. **Refactor Test Code**: Apply same standards as production
4. **Review Test Coverage**: Identify gaps regularly
5. **Update Tests with Features**: Never merge code without tests

### CI/CD Integration
1. **Fast Feedback**: Run unit tests first (fail fast)
2. **Parallel Execution**: Run tests in parallel where possible
3. **Cache Dependencies**: Speed up pipeline with dependency caching
4. **Clear Failure Messages**: Help developers debug quickly
5. **Block Bad Code**: Enforce quality gates (coverage, security)

---

## üîí Compliance & Audit

**Test Audit Trail**:
- All test runs logged (timestamp, commit, results)
- Coverage reports archived (monthly)
- Security scan results retained (12 months)
- Bug reports linked to fixes (traceability)

**Compliance Requirements**:
- 80% code coverage for all services
- Zero high/critical security vulnerabilities
- All P0/P1 bugs have regression tests
- Performance baselines met (SLA compliance)

---

## üìû Support & Questions

**For Test Strategy Questions**:
- Review existing test plans in `/docs/testing/test-plans/`
- Consult Systems Architect for architecture-dependent tests
- Collaborate with BA Agent for acceptance criteria clarification

**For Test Failures**:
- Check CI logs in GitHub Actions workflow runs
- Review test output for specific failure reasons
- Escalate to Systems Architect if systemic issue
- Report to Deployment Agent if blocking deployment

**For Coverage Gaps**:
- Run coverage report locally: `pytest --cov --cov-report=html`
- Identify untested code paths
- Prioritize critical paths (P0/P1 features)
- Create test cases, link to user stories

---

**Last Updated**: January 18, 2026  
**Next Review**: February 18, 2026  
**Agent Status**: ‚úÖ Active
